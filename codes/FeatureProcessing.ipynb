{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select LSI features for the two class classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories_Ct = ['comp.graphics', 'comp.os.ms-windows.misc' ,'comp.sys.ibm.pc.hardware','comp.sys.mac.hardware']\n",
    "categories_Ra = ['rec.autos','rec.motorcycles','rec.sport.baseball','rec.sport.hockey']\n",
    "twenty_train_Ct = fetch_20newsgroups(subset='train', categories=categories_Ct, shuffle=True, random_state=42)\n",
    "twenty_test_Ct = fetch_20newsgroups(subset='test', categories=categories_Ct, shuffle=True, random_state=42)\n",
    "twenty_train_Ra = fetch_20newsgroups(subset='train', categories=categories_Ra, shuffle=True, random_state=42)\n",
    "twenty_test_Ra = fetch_20newsgroups(subset='test', categories=categories_Ra, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 2, ..., 7, 4, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Ra_train_data =twenty_train_Ra.data[:2343]\n",
    "Ra_train_target = map(lambda x: x+4, twenty_train_Ra.target[:2343])\n",
    "Ra_test_target = map(lambda x: x+4, twenty_test_Ra.target)\n",
    "twenty_train_all_X =twenty_train_Ct.data + Ra_train_data\n",
    "twenty_train_all_y = np.array(twenty_train_Ct.target.tolist() + Ra_train_target)\n",
    "\n",
    "twenty_test_all_X =twenty_test_Ct.data + twenty_test_Ra.data\n",
    "twenty_test_all_y = np.array(twenty_test_Ct.target.tolist() + Ra_test_target)\n",
    "twenty_train_all_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def XcountsafterStem(X, vocab,vocab_new):\n",
    "    #vocab_new={}\n",
    "    for i in vocab:\n",
    "        word = stemmer.stem(i)\n",
    "        if word in vocab_new:\n",
    "            vocab_new[word].append(vocab[i])\n",
    "        else:\n",
    "            vocab_new[word]=[vocab[i]]\n",
    "    term_num = len(vocab_new)\n",
    "    doc_num = X.shape[0]\n",
    "    X_new = np.array ([ [ 0 for i in range(term_num)] for j in range(doc_num) ])\n",
    "    for i in range(doc_num):\n",
    "        for j in range(len(vocab_new.keys())):\n",
    "            idxs = vocab_new[vocab_new.keys()[j]]\n",
    "            for idx in idxs:\n",
    "                X_new[i][j] += X[i][idx]\n",
    "    return X_new\n",
    "\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(twenty_train_all_X)\n",
    "X_test_counts = count_vect.transform(twenty_test_all_X) #for test data\n",
    "\n",
    "\n",
    "# dic = count_vect.vocabulary_\n",
    "# newDic={}\n",
    "# X_train_counts = XcountsafterStem(X_train_counts.toarray(), dic,newDic)\n",
    "# newDic={}\n",
    "# X_test_counts = XcountsafterStem(X_test_counts.toarray(), dic,newDic)\n",
    "# print X_train.counts.shape\n",
    "# print X_test_counts.shape\n",
    "# print len(newDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_new={}\n",
    "vocab = count_vect.vocabulary_\n",
    "X = X_train_counts.toarray()\n",
    "for i in vocab:\n",
    "    word = stemmer.stem(i)\n",
    "    if word in vocab_new:\n",
    "        vocab_new[word].append(vocab[i])\n",
    "    else:\n",
    "        vocab_new[word]=[vocab[i]]\n",
    "term_num = len(vocab_new)\n",
    "doc_num = X.shape[0]\n",
    "X_new = np.array ([ [ 0 for i in range(term_num)] for j in range(doc_num) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3150, 78726)\n",
      "69497\n",
      "4686\n"
     ]
    }
   ],
   "source": [
    "X = X_test_counts.toarray()\n",
    "print X.shape\n",
    "print term_num\n",
    "print doc_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "(3150, 69497)\n"
     ]
    }
   ],
   "source": [
    "doc_num = 3150\n",
    "X_new = np.array ([ [ 0 for i in range(term_num)] for j in range(doc_num) ])\n",
    "keys = vocab_new.keys()\n",
    "for i in range(doc_num):\n",
    "    if i%100==0:\n",
    "        print i\n",
    "    for j in range(len(keys)):\n",
    "        idxs = vocab_new[keys[j]]\n",
    "        for idx in idxs:\n",
    "            X_new[i][j]+=X[i][idx]\n",
    "print X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3150, 69497)\n"
     ]
    }
   ],
   "source": [
    "print X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 107143)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from nltk import word_tokenize          \n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# import nltk\n",
    "# stemmer = PorterStemmer()\n",
    "# def stem_tokens(tokens, stemmer):\n",
    "#     stemmed = []\n",
    "#     for item in tokens:\n",
    "#         stemmed.append(stemmer.stem(item))\n",
    "#     return stemmed\n",
    "\n",
    "# def tokenize(text):\n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "#     stems = stem_tokens(tokens, stemmer)\n",
    "#     return stems\n",
    "\n",
    "# count_vect = CountVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "# X_train_counts = count_vect.fit_transform(twenty_train_all_X)\n",
    "# X_test_counts = count_vect.transform(twenty_test_all_X) #for test data\n",
    "# X_train_counts.shape\n",
    "# X_test_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9bbbe45f41a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_train_all_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train = vect.fit_transform(twenty_train_all_X)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4686, 69497)\n"
     ]
    }
   ],
   "source": [
    "X_train_counts = X_new\n",
    "print X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3150, 69497)\n"
     ]
    }
   ],
   "source": [
    "X_test_counts = X_new\n",
    "print X_test_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4686, 69497)\n",
      "(3150, 69497)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "print X_train_tfidf.shape\n",
    "print X_test_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4686, 50)\n",
      "(3150, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "svd.fit(X_train_tfidf)\n",
    "X_train_LSI = svd.transform(X_train_tfidf)\n",
    "X_test_LSI = svd.transform(X_test_tfidf)\n",
    "print X_train_LSI.shape\n",
    "print X_test_LSI.shape\n",
    "##################Finished Feature selectsion################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x>3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "y_train_LSI = np.array(map(f,twenty_train_all_y))\n",
    "y_test_LSI = np.array(map(f,twenty_test_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.974603174603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train_LSI,y_train_LSI)\n",
    "y_test_predicted = svm_model.predict(X_test_LSI)\n",
    "print 'accuracy: ', 1.0*sum(y_test_LSI==y_test_predicted) / len(y_test_LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###save X and y\n",
    "np.save('./data/X_train_LSI', X_train_LSI)\n",
    "np.save('./data/X_test_LSI', X_test_LSI)\n",
    "np.save('./data/y_train_LSI', y_train_LSI)\n",
    "np.save('./data/y_test_LSI', y_test_LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featch data and select features for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "cat = ['comp.sys.ibm.pc.hardware','comp.sys.mac.hardware','misc.forsale','soc.religion.christian']\n",
    "twenty_train_multi = fetch_20newsgroups(subset='train', categories=cat, shuffle=True, random_state=42)\n",
    "twenty_test_multi = fetch_20newsgroups(subset='test', categories=cat, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2352\n",
      "2352\n",
      "2352\n",
      "4\n",
      "['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'misc.forsale', 'soc.religion.christian']\n",
      "[3 2 0 2 3 0 2 3 0 1 2 1 1 2 0 2 1 3 1 0 0 3 0 1 1 1 1 2 3 3 0 2 3 0 2 0 1\n",
      " 3 2 3 2 3 0 1 3 3 2 1 3 0]\n"
     ]
    }
   ],
   "source": [
    "print len(twenty_train_multi.data)\n",
    "print len(twenty_train_multi.filenames)\n",
    "print len(twenty_train_multi.target)\n",
    "print len(twenty_train_multi.target_names)\n",
    "print twenty_train_multi.target_names\n",
    "print twenty_train_multi.target[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2352, 31537)\n",
      "(1565, 31537)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "count_vect_multi = CountVectorizer(stop_words='english')\n",
    "X_train_counts_multi = count_vect_multi.fit_transform(twenty_train_multi.data)\n",
    "X_test_counts_multi = count_vect_multi.transform(twenty_test_multi.data) #for test data\n",
    "print X_train_counts_multi.shape\n",
    "print X_test_counts_multi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25031\n"
     ]
    }
   ],
   "source": [
    "vocab_new={}\n",
    "vocab = count_vect_multi.vocabulary_\n",
    "for i in vocab:\n",
    "    word = stemmer.stem(i)\n",
    "    if word in vocab_new:\n",
    "        vocab_new[word].append(vocab[i])\n",
    "    else:\n",
    "        vocab_new[word]=[vocab[i]]\n",
    "term_num = len(vocab_new)\n",
    "print term_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X_train_counts_multi.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "(2352, 25031)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = X_train_counts_multi.toarray()\n",
    "doc_num = X.shape[0]\n",
    "X_new = np.array ([ [ 0 for i in range(term_num)] for j in range(doc_num) ])\n",
    "keys = vocab_new.keys()\n",
    "for i in range(doc_num):\n",
    "    if i%100==0:\n",
    "        print i\n",
    "    for j in range(len(keys)):\n",
    "        idxs = vocab_new[keys[j]]\n",
    "        for idx in idxs:\n",
    "            X_new[i][j]+=X[i][idx]\n",
    "print X_new.shape\n",
    "X_train_counts_multi = X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "(1565, 25031)\n"
     ]
    }
   ],
   "source": [
    "X = X_test_counts_multi.toarray()\n",
    "doc_num = X.shape[0]\n",
    "X_new = np.array ([ [ 0 for i in range(term_num)] for j in range(doc_num) ])\n",
    "keys = vocab_new.keys()\n",
    "for i in range(doc_num):\n",
    "    if i%100==0:\n",
    "        print i\n",
    "    for j in range(len(keys)):\n",
    "        idxs = vocab_new[keys[j]]\n",
    "        for idx in idxs:\n",
    "            X_new[i][j]+=X[i][idx]\n",
    "print X_new.shape\n",
    "X_test_counts_multi = X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2352, 25031)\n",
      "(1565, 25031)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf_multi = tfidf_transformer.fit_transform(X_train_counts_multi)\n",
    "X_test_tfidf_multi = tfidf_transformer.transform(X_test_counts_multi)\n",
    "print X_train_tfidf_multi.shape\n",
    "print X_test_tfidf_multi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2352, 50)\n",
      "(1565, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_multi = TruncatedSVD(n_components=50, random_state=42)\n",
    "svd_multi.fit(X_train_tfidf_multi)\n",
    "X_train_LSI_multi = svd_multi.transform(X_train_tfidf_multi)\n",
    "X_test_LSI_multi = svd_multi.transform(X_test_tfidf_multi)\n",
    "print X_train_LSI_multi.shape\n",
    "print X_test_LSI_multi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.877955271565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "label_test = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X_train_LSI_multi, twenty_train_multi.target).predict(X_test_LSI_multi)\n",
    "print 'accuracy: ', 1.0*sum(label_test==twenty_test_multi.target) / twenty_test_multi.target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###save X and y for multi-class classification\n",
    "np.save('./data/X_train_LSI_multi', X_train_LSI_multi)\n",
    "np.save('./data/X_test_LSI_multi', X_test_LSI_multi)\n",
    "np.save('./data/y_train_LSI_multi', twenty_train_multi.target)\n",
    "np.save('./data/y_test_LSI_multi', twenty_test_multi.target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
